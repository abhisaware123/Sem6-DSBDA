{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34b9d77",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "  1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, \n",
    "     stop words removal, Stemming and Lemmatization.\n",
    "  2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "     Frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77ac21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2bb822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\C2K19\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\C2K19\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\C2K19\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\C2K19\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcaa7540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "corpus = \"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062bb838",
   "metadata": {},
   "source": [
    "# Task1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384bb6d",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719edc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b65c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8975bea",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cbbc30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard\"]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = sent_tokenize(corpus)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5994a8c",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c933351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word = word_tokenize(corpus)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558f1bf",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc234904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7289782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'re', 'where', 'his', 'doesn', 'was', 'now', \"aren't\", 'out', \"mustn't\", 'doing', 'of', 'won', 'are', 'about', 'over', 'been', 'can', 's', 'which', 'at', 'to', 'but', 'with', 'o', 'from', 'off', \"hasn't\", \"she's\", \"you'll\", \"don't\", 'herself', 'down', 'most', \"didn't\", 'should', \"needn't\", 'or', 'once', \"mightn't\", \"wouldn't\", 'for', 'on', 'does', 'and', 'under', 'only', 'yourself', 'so', 'there', \"isn't\", 'above', 'why', 'had', 'am', \"shouldn't\", 'all', 'as', 'more', 'couldn', 'in', 'them', 'i', 'against', 'your', 'me', 'he', 'hasn', 'did', 'you', 'an', 'by', 'below', 'here', 'didn', 'needn', 'shouldn', 'being', 'yourselves', 'itself', 'other', 'this', 'no', 'y', 'll', 'yours', \"weren't\", 'don', 'ourselves', \"you've\", 'when', \"you'd\", 'm', 'these', 'if', 'who', \"hadn't\", \"couldn't\", 'it', 'is', 'himself', 'aren', 'isn', 'mightn', 'while', \"won't\", \"haven't\", 'theirs', 'the', \"you're\", 'him', 'my', 'ours', 'have', 'after', 'own', 'weren', 'then', 'we', 'between', 'some', 'during', 'having', 'she', 'be', 'than', 'how', 'hers', 'same', 'mustn', \"doesn't\", 'whom', 'into', 'themselves', 'has', 'such', 'through', 'those', 'were', 'ma', 'will', 'that', 'few', 'd', 'its', 'haven', 'they', 'our', \"shan't\", 'up', 'a', 'until', 'both', 'just', \"should've\", 'because', 'ain', 'hadn', \"wasn't\", 'each', 'what', \"it's\", 'again', 'shan', 'before', 'do', 'further', 'myself', 'any', 'very', 'wasn', 'wouldn', 'their', 've', 't', 'not', 'nor', 'too', \"that'll\", 'her'}\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85df8357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n",
      "\n",
      "\n",
      "Filterd Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent = []\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Tokenized words:\",tokenized_word)\n",
    "print('\\n')\n",
    "print(\"Filterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7bb47",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebb98dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb686221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "\n",
      "\n",
      "Stemmed Sentence: ['hello', 'mr.', 'smith', ',', 'today', '?', 'the', 'weather', 'great', ',', 'citi', 'awesom', '.', 'the', 'sky', 'pinkish-blu', '.', 'you', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_words = []\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print('\\n')\n",
    "print(\"Stemmed Sentence:\",stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc4f7b",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faeb8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "059f6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "\n",
      "\n",
      "Stemmed Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "\n",
    "for w in filtered_sent:\n",
    "    lemmatized_words.append(lem.lemmatize(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print('\\n')\n",
    "print(\"Stemmed Sentence:\",lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0150fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeaa4d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449c989",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5897bc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Smith', 'NNP'),\n",
       " (',', ','),\n",
       " ('how', 'WRB'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('doing', 'VBG'),\n",
       " ('today', 'NN'),\n",
       " ('?', '.'),\n",
       " ('The', 'DT'),\n",
       " ('weather', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('great', 'JJ'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('city', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('awesome', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('sky', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('pinkish-blue', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('You', 'PRP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('eat', 'VB'),\n",
       " ('cardboard', 'NN')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba34473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eb397ab",
   "metadata": {},
   "source": [
    "# Task2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afbcfd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'science', 'Data', 'learning', 'key', 'Science', 'machine', 'data', 'century', 'is', 'job', 'of', 'the', '21st', 'for', 'hardest'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence = \"Data Science is the hardest job of the 21st century\"\n",
    "second_sentence = \"machine learning is the key for data science\"\n",
    "\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")\n",
    "\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b75f5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'science': 0, 'Data': 1, 'learning': 0, 'key': 0, 'Science': 1, 'machine': 0, 'data': 0, 'century': 1, 'is': 1, 'job': 1, 'of': 1, 'the': 2, '21st': 1, 'for': 0, 'hardest': 1}\n",
      "{'science': 1, 'Data': 0, 'learning': 1, 'key': 1, 'Science': 0, 'machine': 1, 'data': 1, 'century': 0, 'is': 1, 'job': 0, 'of': 0, 'the': 1, '21st': 0, 'for': 1, 'hardest': 0}\n"
     ]
    }
   ],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "print(wordDictA)\n",
    "print(wordDictB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ee0fb55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>science</th>\n",
       "      <th>Data</th>\n",
       "      <th>learning</th>\n",
       "      <th>key</th>\n",
       "      <th>Science</th>\n",
       "      <th>machine</th>\n",
       "      <th>data</th>\n",
       "      <th>century</th>\n",
       "      <th>is</th>\n",
       "      <th>job</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>21st</th>\n",
       "      <th>for</th>\n",
       "      <th>hardest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   science  Data  learning  key  Science  machine  data  century  is  job  of  \\\n",
       "0        0     1         0    0        1        0     0        1   1    1   1   \n",
       "1        1     0         1    1        0        1     1        0   1    0   0   \n",
       "\n",
       "   the  21st  for  hardest  \n",
       "0    2     1    0        1  \n",
       "1    1     0    1        0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf6275",
   "metadata": {},
   "source": [
    "# TF (Term Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17a3463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6313b905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   science  Data  learning    key  Science  machine   data  century     is  \\\n",
      "0    0.000   0.1     0.000  0.000      0.1    0.000  0.000      0.1  0.100   \n",
      "1    0.125   0.0     0.125  0.125      0.0    0.125  0.125      0.0  0.125   \n",
      "\n",
      "   job   of    the  21st    for  hardest  \n",
      "0  0.1  0.1  0.200   0.1  0.000      0.1  \n",
      "1  0.0  0.0  0.125   0.0  0.125      0.0  \n"
     ]
    }
   ],
   "source": [
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e60f8d",
   "metadata": {},
   "source": [
    "# IDF (Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e243323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        cnt = 0\n",
    "        for doc in docList:\n",
    "            if(doc[word] != 0):\n",
    "                cnt += 1\n",
    "        idfDict[word] = cnt\n",
    "#     print(idfDict)\n",
    "        \n",
    "        \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) ))\n",
    "        \n",
    "    return(idfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e6d6220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'science': 0.3010299956639812, 'Data': 0.3010299956639812, 'learning': 0.3010299956639812, 'key': 0.3010299956639812, 'Science': 0.3010299956639812, 'machine': 0.3010299956639812, 'data': 0.3010299956639812, 'century': 0.3010299956639812, 'is': 0.0, 'job': 0.3010299956639812, 'of': 0.3010299956639812, 'the': 0.0, '21st': 0.3010299956639812, 'for': 0.3010299956639812, 'hardest': 0.3010299956639812}\n"
     ]
    }
   ],
   "source": [
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "print(idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d52663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
